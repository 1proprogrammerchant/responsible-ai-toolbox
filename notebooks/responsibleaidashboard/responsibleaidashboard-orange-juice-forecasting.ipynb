{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Responsible AI dashboard for Time Series Forecasting\n",
        "_**Orange Juice Sales Forecasting**_\n",
        "\n",
        "## Contents\n",
        "1. [Introduction](#introduction)\n",
        "1. [Data](#data)\n",
        "1. [Train](#train)\n",
        "1. [Forecast](#forecast)\n",
        "1. [Responsible AI Dashboard](#analyze)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction<a id=\"introduction\"></a>\n",
        "In this example, we use sktime to train, select, and operationalize a time-series forecasting model for multiple time-series.\n",
        "\n",
        "The examples in the follow code samples use the University of Chicago's Dominick's Finer Foods dataset to forecast orange juice sales. Dominick's was a grocery chain in the Chicago metropolitan area."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1670990788014
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import PowerTransformer, RobustScaler, MinMaxScaler\n",
        "from sktime.forecasting.compose import TransformedTargetForecaster\n",
        "from sktime.transformations.series.adapt import TabularToSeriesAdaptor\n",
        "from sktime.transformations.series.detrend import Deseasonalizer, Detrender\n",
        "from sktime.forecasting.arima import AutoARIMA\n",
        "from sktime.forecasting.base import ForecastingHorizon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data<a id=\"data\"></a>\n",
        "You are now ready to load the historical orange juice sales data. We will load the CSV file into a plain pandas DataFrame; the time column in the CSV is called _WeekStarting_, so it will be specially parsed into the datetime type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1670990899201
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Quantity</th>\n",
              "      <th>Advert</th>\n",
              "      <th>Price</th>\n",
              "      <th>Age60</th>\n",
              "      <th>COLLEGE</th>\n",
              "      <th>INCOME</th>\n",
              "      <th>Hincome150</th>\n",
              "      <th>Large HH</th>\n",
              "      <th>Minorities</th>\n",
              "      <th>WorkingWoman</th>\n",
              "      <th>SSTRDIST</th>\n",
              "      <th>SSTRVOL</th>\n",
              "      <th>CPDIST5</th>\n",
              "      <th>CPWVOL5</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Store</th>\n",
              "      <th>Brand</th>\n",
              "      <th>WeekStarting</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
              "      <th>dominicks</th>\n",
              "      <th>1990-06-14</th>\n",
              "      <td>10560</td>\n",
              "      <td>1</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.232865</td>\n",
              "      <td>0.248935</td>\n",
              "      <td>10.553205</td>\n",
              "      <td>0.463887</td>\n",
              "      <td>0.103953</td>\n",
              "      <td>0.114280</td>\n",
              "      <td>0.303585</td>\n",
              "      <td>2.110122</td>\n",
              "      <td>1.142857</td>\n",
              "      <td>1.927280</td>\n",
              "      <td>0.376927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>minute.maid</th>\n",
              "      <th>1990-06-14</th>\n",
              "      <td>4480</td>\n",
              "      <td>0</td>\n",
              "      <td>3.17</td>\n",
              "      <td>0.232865</td>\n",
              "      <td>0.248935</td>\n",
              "      <td>10.553205</td>\n",
              "      <td>0.463887</td>\n",
              "      <td>0.103953</td>\n",
              "      <td>0.114280</td>\n",
              "      <td>0.303585</td>\n",
              "      <td>2.110122</td>\n",
              "      <td>1.142857</td>\n",
              "      <td>1.927280</td>\n",
              "      <td>0.376927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tropicana</th>\n",
              "      <th>1990-06-14</th>\n",
              "      <td>8256</td>\n",
              "      <td>0</td>\n",
              "      <td>3.87</td>\n",
              "      <td>0.232865</td>\n",
              "      <td>0.248935</td>\n",
              "      <td>10.553205</td>\n",
              "      <td>0.463887</td>\n",
              "      <td>0.103953</td>\n",
              "      <td>0.114280</td>\n",
              "      <td>0.303585</td>\n",
              "      <td>2.110122</td>\n",
              "      <td>1.142857</td>\n",
              "      <td>1.927280</td>\n",
              "      <td>0.376927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
              "      <th>dominicks</th>\n",
              "      <th>1990-06-14</th>\n",
              "      <td>1792</td>\n",
              "      <td>1</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.117368</td>\n",
              "      <td>0.321226</td>\n",
              "      <td>10.922371</td>\n",
              "      <td>0.535883</td>\n",
              "      <td>0.103092</td>\n",
              "      <td>0.053875</td>\n",
              "      <td>0.410568</td>\n",
              "      <td>3.801998</td>\n",
              "      <td>0.681818</td>\n",
              "      <td>1.600573</td>\n",
              "      <td>0.736307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>minute.maid</th>\n",
              "      <th>1990-06-14</th>\n",
              "      <td>4224</td>\n",
              "      <td>0</td>\n",
              "      <td>2.99</td>\n",
              "      <td>0.117368</td>\n",
              "      <td>0.321226</td>\n",
              "      <td>10.922371</td>\n",
              "      <td>0.535883</td>\n",
              "      <td>0.103092</td>\n",
              "      <td>0.053875</td>\n",
              "      <td>0.410568</td>\n",
              "      <td>3.801998</td>\n",
              "      <td>0.681818</td>\n",
              "      <td>1.600573</td>\n",
              "      <td>0.736307</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                Quantity  Advert  Price     Age60   COLLEGE  \\\n",
              "Store Brand       WeekStarting                                                \n",
              "2     dominicks   1990-06-14       10560       1   1.59  0.232865  0.248935   \n",
              "      minute.maid 1990-06-14        4480       0   3.17  0.232865  0.248935   \n",
              "      tropicana   1990-06-14        8256       0   3.87  0.232865  0.248935   \n",
              "5     dominicks   1990-06-14        1792       1   1.59  0.117368  0.321226   \n",
              "      minute.maid 1990-06-14        4224       0   2.99  0.117368  0.321226   \n",
              "\n",
              "                                   INCOME  Hincome150  Large HH  Minorities  \\\n",
              "Store Brand       WeekStarting                                                \n",
              "2     dominicks   1990-06-14    10.553205    0.463887  0.103953    0.114280   \n",
              "      minute.maid 1990-06-14    10.553205    0.463887  0.103953    0.114280   \n",
              "      tropicana   1990-06-14    10.553205    0.463887  0.103953    0.114280   \n",
              "5     dominicks   1990-06-14    10.922371    0.535883  0.103092    0.053875   \n",
              "      minute.maid 1990-06-14    10.922371    0.535883  0.103092    0.053875   \n",
              "\n",
              "                                WorkingWoman  SSTRDIST   SSTRVOL   CPDIST5  \\\n",
              "Store Brand       WeekStarting                                               \n",
              "2     dominicks   1990-06-14        0.303585  2.110122  1.142857  1.927280   \n",
              "      minute.maid 1990-06-14        0.303585  2.110122  1.142857  1.927280   \n",
              "      tropicana   1990-06-14        0.303585  2.110122  1.142857  1.927280   \n",
              "5     dominicks   1990-06-14        0.410568  3.801998  0.681818  1.600573   \n",
              "      minute.maid 1990-06-14        0.410568  3.801998  0.681818  1.600573   \n",
              "\n",
              "                                 CPWVOL5  \n",
              "Store Brand       WeekStarting            \n",
              "2     dominicks   1990-06-14    0.376927  \n",
              "      minute.maid 1990-06-14    0.376927  \n",
              "      tropicana   1990-06-14    0.376927  \n",
              "5     dominicks   1990-06-14    0.736307  \n",
              "      minute.maid 1990-06-14    0.736307  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time_column_name = \"WeekStarting\"\n",
        "time_series_id_column_names = [\"Store\", \"Brand\"]\n",
        "dataset_location = \"https://raw.githubusercontent.com/Azure/azureml-examples/2fe81643865e1f4591e7734bd1a729093cafb826/v1/python-sdk/tutorials/automl-with-azureml/forecasting-orange-juice-sales/dominicks_OJ.csv\"\n",
        "data = pd.read_csv(dataset_location, parse_dates=[time_column_name])\n",
        "\n",
        "# Drop the columns 'logQuantity' as it is a leaky feature.\n",
        "data.drop(\"logQuantity\", axis=1, inplace=True)\n",
        "\n",
        "data.set_index(time_series_id_column_names + [time_column_name], inplace=True, ascending=[True, True, True])\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each row in the DataFrame holds a quantity of weekly sales for an OJ brand at a single store. The data also includes the sales price, a flag indicating if the OJ brand was advertised in the store that week, and some customer demographic information based on the store location. For historical reasons, the data also include the logarithm of the sales quantity. The Dominick's grocery data is commonly used to illustrate econometric modeling techniques where logarithms of quantities are generally preferred.    \n",
        "\n",
        "The task is now to build a time-series model for the _Quantity_ column. It is important to note that this dataset is comprised of many individual time-series - one for each unique combination of _Store_ and _Brand_. To distinguish the individual time-series, we define the **time_series_id_column_names** - the columns whose values determine the boundaries between time-series: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1670990902872
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data contains 249 individual time-series.\n"
          ]
        },
        {
          "ename": "UnsortedIndexError",
          "evalue": "'MultiIndex slicing requires the index to be lexsorted: slicing on levels [0], lexsort depth 0'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnsortedIndexError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[49], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m nseries \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mgroupby(time_series_id_column_names)\u001b[39m.\u001b[39mngroups\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mData contains \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m individual time-series.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(nseries))\n\u001b[1;32m----> 3\u001b[0m data\u001b[39m.\u001b[39;49mloc[(\u001b[39mslice\u001b[39;49m(\u001b[39m2\u001b[39;49m,\u001b[39m5\u001b[39;49m), \u001b[39m\"\u001b[39;49m\u001b[39mdominicks\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m1990-06-14\u001b[39;49m\u001b[39m\"\u001b[39;49m)]\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\indexing.py:1067\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1066\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[1;32m-> 1067\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[0;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1069\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\indexing.py:1247\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[39mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m   1246\u001b[0m     tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_ellipsis(tup)\n\u001b[1;32m-> 1247\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_lowerdim(tup)\n\u001b[0;32m   1249\u001b[0m \u001b[39m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[0;32m   1250\u001b[0m tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_tuple_indexer(tup)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\indexing.py:941\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[39m# we may have a nested tuples indexer here\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_nested_tuple_indexer(tup):\n\u001b[1;32m--> 941\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_nested_tuple(tup)\n\u001b[0;32m    943\u001b[0m \u001b[39m# we maybe be using a tuple to represent multiple dimensions here\u001b[39;00m\n\u001b[0;32m    944\u001b[0m ax0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(\u001b[39m0\u001b[39m)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\indexing.py:1033\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_nested_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[39m# this is a series with a multi-index specified a tuple of\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m     \u001b[39m# selectors\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1033\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(tup, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1035\u001b[0m \u001b[39m# handle the multi-axis by taking sections and reducing\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[39m# this is iterative\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\indexing.py:1305\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1303\u001b[0m \u001b[39m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m \u001b[39mif\u001b[39;00m is_nested_tuple(key, labels):\n\u001b[1;32m-> 1305\u001b[0m     locs \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39;49mget_locs(key)\n\u001b[0;32m   1306\u001b[0m     indexer \u001b[39m=\u001b[39m [\u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m)] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim\n\u001b[0;32m   1307\u001b[0m     indexer[axis] \u001b[39m=\u001b[39m locs\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\indexes\\multi.py:3331\u001b[0m, in \u001b[0;36mMultiIndex.get_locs\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m   3329\u001b[0m true_slices \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m (i, s) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(com\u001b[39m.\u001b[39mis_true_slices(seq)) \u001b[39mif\u001b[39;00m s]\n\u001b[0;32m   3330\u001b[0m \u001b[39mif\u001b[39;00m true_slices \u001b[39mand\u001b[39;00m true_slices[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lexsort_depth:\n\u001b[1;32m-> 3331\u001b[0m     \u001b[39mraise\u001b[39;00m UnsortedIndexError(\n\u001b[0;32m   3332\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMultiIndex slicing requires the index to be lexsorted: slicing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3333\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mon levels \u001b[39m\u001b[39m{\u001b[39;00mtrue_slices\u001b[39m}\u001b[39;00m\u001b[39m, lexsort depth \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lexsort_depth\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3334\u001b[0m     )\n\u001b[0;32m   3336\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(x \u001b[39mis\u001b[39;00m \u001b[39mEllipsis\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m seq):\n\u001b[0;32m   3337\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   3338\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMultiIndex does not support indexing with Ellipsis\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3339\u001b[0m     )\n",
            "\u001b[1;31mUnsortedIndexError\u001b[0m: 'MultiIndex slicing requires the index to be lexsorted: slicing on levels [0], lexsort depth 0'"
          ]
        }
      ],
      "source": [
        "nseries = data.groupby(time_series_id_column_names).ngroups\n",
        "print(\"Data contains {0} individual time-series.\".format(nseries))\n",
        "data.loc[(slice(2,5), \"dominicks\", \"1990-06-14\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For demonstration purposes, we extract sales time-series for just a few of the stores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1670990905562
        }
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'Store'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m use_stores \u001b[39m=\u001b[39m [\u001b[39m2\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m8\u001b[39m]\n\u001b[0;32m      2\u001b[0m data_subset \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mloc[\u001b[39m2\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m nseries \u001b[39m=\u001b[39m data_subset\u001b[39m.\u001b[39;49mgroupby(time_series_id_column_names)\u001b[39m.\u001b[39mngroups\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mData subset contains \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m individual time-series.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(nseries))\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\frame.py:8402\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   8399\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   8400\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8402\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   8403\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   8404\u001b[0m     keys\u001b[39m=\u001b[39;49mby,\n\u001b[0;32m   8405\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   8406\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   8407\u001b[0m     as_index\u001b[39m=\u001b[39;49mas_index,\n\u001b[0;32m   8408\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   8409\u001b[0m     group_keys\u001b[39m=\u001b[39;49mgroup_keys,\n\u001b[0;32m   8410\u001b[0m     squeeze\u001b[39m=\u001b[39;49msqueeze,\n\u001b[0;32m   8411\u001b[0m     observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m   8412\u001b[0m     dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[0;32m   8413\u001b[0m )\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:965\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgroupby\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrouper\u001b[39;00m \u001b[39mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 965\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[0;32m    966\u001b[0m         obj,\n\u001b[0;32m    967\u001b[0m         keys,\n\u001b[0;32m    968\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    969\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    970\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    971\u001b[0m         observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m    972\u001b[0m         mutated\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmutated,\n\u001b[0;32m    973\u001b[0m         dropna\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropna,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    976\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m obj\n\u001b[0;32m    977\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis_number(axis)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    886\u001b[0m         in_axis, level, gpr \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, gpr, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    889\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(gpr, Grouper) \u001b[39mand\u001b[39;00m gpr\u001b[39m.\u001b[39mkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[39m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     exclusions\u001b[39m.\u001b[39madd(gpr\u001b[39m.\u001b[39mkey)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'Store'"
          ]
        }
      ],
      "source": [
        "use_stores = [2, 5, 8]\n",
        "data_subset = data.loc[2]\n",
        "nseries = data_subset.groupby(time_series_id_column_names).ngroups\n",
        "print(\"Data subset contains {0} individual time-series.\".format(nseries))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Splitting\n",
        "We now split the data into a training and a testing set for later forecast evaluation. The test set will contain the final 20 weeks of observed sales for each time-series. The splits should be stratified by series, so we use a group-by statement on the time series identifier columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1670990907583
        }
      },
      "outputs": [],
      "source": [
        "n_test_periods = 20\n",
        "\n",
        "def split_last_n_by_series_id(df, n):\n",
        "    \"\"\"Group df by series identifiers and split on last n rows for each group.\"\"\"\n",
        "    df_grouped = df.sort_values(time_column_name).groupby(  # Sort by ascending time\n",
        "        time_series_id_column_names, group_keys=False\n",
        "    )\n",
        "    df_head = df_grouped.apply(lambda dfg: dfg.iloc[:-n])\n",
        "    df_tail = df_grouped.apply(lambda dfg: dfg.iloc[-n:])\n",
        "    return df_head, df_tail\n",
        "\n",
        "train, test = split_last_n_by_series_id(data_subset, n_test_periods)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modeling\n",
        "\n",
        "For forecasting tasks, we need to do several pre-processing and estimation steps that are specific to time-series including:\n",
        "* Detect time-series sample frequency (e.g. hourly, daily, weekly) and create new records for absent time points to make the series regular. A regular time series has a well-defined frequency and has a value at every sample point in a contiguous time span \n",
        "* Impute missing values in the target (via forward-fill) and feature columns (using median column values) \n",
        "* Create features based on time series identifiers to enable fixed effects across different series\n",
        "* Create time-based features to assist in learning seasonal patterns\n",
        "* Encode categorical variables to numeric quantities\n",
        "\n",
        "In this notebook, we will train a single, regression-type model across **all** time-series in a given training set. This allows the model to generalize across related series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1670990924966
        }
      },
      "outputs": [],
      "source": [
        "target_column_name = \"Quantity\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Customization\n",
        "\n",
        "The featurization customization in forecasting is an advanced feature in AutoML which allows our customers to change the default forecasting featurization behaviors and column types through `FeaturizationConfig`. The supported scenarios include:\n",
        "\n",
        "1. Column purposes update: Override feature type for the specified column. Currently supports DateTime, Categorical and Numeric. This customization can be used in the scenario that the type of the column cannot correctly reflect its purpose. Some numerical columns, for instance, can be treated as Categorical columns which need to be converted to categorical while some can be treated as epoch timestamp which need to be converted to datetime. To tell our SDK to correctly preprocess these columns, a configuration need to be add with the columns and their desired types.\n",
        "2. Transformer parameters update: Currently supports parameter change for Imputer only. User can customize imputation methods. The supported imputing methods for target column are constant and ffill (forward fill). The supported imputing methods for feature columns are mean, median, most frequent, constant and ffill (forward fill). This customization can be used for the scenario that our customers know which imputation methods fit best to the input data. For instance, some datasets use NaN to represent 0 which the correct behavior should impute all the missing value with 0. To achieve this behavior, these columns need to be configured as constant imputation with `fill_value` 0.\n",
        "3. Drop columns: Columns to drop from being featurized. These usually are the columns which are leaky or the columns contain no useful data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1670990927322
        },
        "tags": [
          "sample-featurizationconfig-remarks"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ForecastingHorizon(['1992-05-14', '1992-05-21', '1992-05-28', '1992-06-04',\n",
              "               '1992-06-11', '1992-06-18', '1992-06-25', '1992-07-02',\n",
              "               '1992-07-09', '1992-07-16', '1992-07-23', '1992-07-30',\n",
              "               '1992-08-06', '1992-08-13', '1992-08-20', '1992-08-27',\n",
              "               '1992-09-03', '1992-09-10', '1992-09-17', '1992-09-24',\n",
              "               '1992-10-01'],\n",
              "              dtype='datetime64[ns]', freq=None, is_relative=False)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "forecaster = TransformedTargetForecaster(steps=[\n",
        "    (\"detrend\", Detrender()),\n",
        "    (\"deseasonalize\", Deseasonalizer()),\n",
        "    (\"minmax\", TabularToSeriesAdaptor(MinMaxScaler((1, 10)))),\n",
        "    (\"power\", TabularToSeriesAdaptor(PowerTransformer())),\n",
        "    (\"scaler\", TabularToSeriesAdaptor(RobustScaler())),\n",
        "    (\"forecaster\", AutoARIMA(sp=1, suppress_warnings=True)),\n",
        "])\n",
        "\n",
        "fh = ForecastingHorizon(pd.DatetimeIndex(np.unique(test[time_column_name])), is_relative=False)\n",
        "\n",
        "\"\"\"\n",
        "featurization_config = FeaturizationConfig()\n",
        "# Force the CPWVOL5 feature to be numeric type.\n",
        "featurization_config.add_column_purpose(\"CPWVOL5\", \"Numeric\")\n",
        "# Fill missing values in the target column, Quantity, with zeros.\n",
        "featurization_config.add_transformer_params(\n",
        "    \"Imputer\", [\"Quantity\"], {\"strategy\": \"constant\", \"fill_value\": 0}\n",
        ")\n",
        "# Fill missing values in the INCOME column with median value.\n",
        "featurization_config.add_transformer_params(\n",
        "    \"Imputer\", [\"INCOME\"], {\"strategy\": \"median\"}\n",
        ")\n",
        "# Fill missing values in the Price column with forward fill (last value carried forward).\n",
        "featurization_config.add_transformer_params(\"Imputer\", [\"Price\"], {\"strategy\": \"ffill\"})\n",
        "\"\"\"\n",
        "fh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forecasting Parameters\n",
        "To define forecasting parameters for your experiment training, you can leverage the ForecastingParameters class. The table below details the forecasting parameter we will be passing into our experiment.\n",
        "\n",
        "\n",
        "|Property|Description|\n",
        "|-|-|\n",
        "|**time_column_name**|The name of your time column.|\n",
        "|**forecast_horizon**|The forecast horizon is how many periods forward you would like to forecast. This integer horizon is in units of the timeseries frequency (e.g. daily, weekly).|\n",
        "|**time_series_id_column_names**|The column names used to uniquely identify the time series in data that has multiple rows with the same timestamp. If the time series identifiers are not defined, the data set is assumed to be one time series.|\n",
        "|**freq**|Forecast frequency. This optional parameter represents the period with which the forecast is desired, for example, daily, weekly, yearly, etc. Use this parameter for the correction of time series containing irregular data points or for padding of short time series. The frequency needs to be a pandas offset alias. Please refer to [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects) for more information.\n",
        "|**cv_step_size**|Number of periods between two consecutive cross-validation folds. The default value is \"auto\", in which case AutoMl determines the cross-validation step size automatically, if a validation set is not provided. Or users could specify an integer value."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train<a id=\"train\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now submit a new training run. Depending on the data and number of iterations this operation may take several minutes.\n",
        "Information from each iteration will be printed to the console.  Validation errors and current status will be shown when setting `show_output=True` and the execution will be synchronous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "y must be in an sktime compatible format, of scitype Series, Panel or Hierarchical, for instance a pandas.DataFrame with sktime compatible time indices, or with MultiIndex and last(-1) level an sktime compatible time index. See the forecasting tutorial examples/01_forecasting.ipynb, or the data format tutorial examples/AA_datatypes_and_datasets.ipynb,If you think y is already in an sktime supported input format, run sktime.datatypes.check_raise(y, mtype) to diagnose the error, where mtype is the string of the type specification you want for y. Possible mtype specification strings are as follows. \"For Hierarchical scitype: ['pd_multiindex_hier']. ",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m forecaster\u001b[39m.\u001b[39;49mfit(y\u001b[39m=\u001b[39;49mtrain[target_column_name], X\u001b[39m=\u001b[39;49mtrain\u001b[39m.\u001b[39;49mdrop(columns\u001b[39m=\u001b[39;49m[target_column_name]), fh\u001b[39m=\u001b[39;49mfh)\n\u001b[0;32m      2\u001b[0m forecaster\u001b[39m.\u001b[39mpredict(fh, test\u001b[39m.\u001b[39mdrop(target_column_name))\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\sktime\\forecasting\\base\\_base.py:334\u001b[0m, in \u001b[0;36mBaseForecaster.fit\u001b[1;34m(self, y, X, fh)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m    333\u001b[0m \u001b[39m# check and convert X/y\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m X_inner, y_inner \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_X_y(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my)\n\u001b[0;32m    336\u001b[0m \u001b[39m# set internal X/y to the new X/y\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[39m# this also updates cutoff from y\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_y_X(y_inner, X_inner)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\sktime\\forecasting\\base\\_base.py:1338\u001b[0m, in \u001b[0;36mBaseForecaster._check_X_y\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   1325\u001b[0m msg \u001b[39m=\u001b[39m (\n\u001b[0;32m   1326\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39my must be in an sktime compatible format, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1327\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mof scitype Series, Panel or Hierarchical, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1335\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPossible mtype specification strings are as follows. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1336\u001b[0m )\n\u001b[0;32m   1337\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m y_valid:\n\u001b[1;32m-> 1338\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg \u001b[39m+\u001b[39m mtypes_msg)\n\u001b[0;32m   1340\u001b[0m y_scitype \u001b[39m=\u001b[39m y_metadata[\u001b[39m\"\u001b[39m\u001b[39mscitype\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1341\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_y_mtype_last_seen \u001b[39m=\u001b[39m y_metadata[\u001b[39m\"\u001b[39m\u001b[39mmtype\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "\u001b[1;31mTypeError\u001b[0m: y must be in an sktime compatible format, of scitype Series, Panel or Hierarchical, for instance a pandas.DataFrame with sktime compatible time indices, or with MultiIndex and last(-1) level an sktime compatible time index. See the forecasting tutorial examples/01_forecasting.ipynb, or the data format tutorial examples/AA_datatypes_and_datasets.ipynb,If you think y is already in an sktime supported input format, run sktime.datatypes.check_raise(y, mtype) to diagnose the error, where mtype is the string of the type specification you want for y. Possible mtype specification strings are as follows. \"For Hierarchical scitype: ['pd_multiindex_hier']. "
          ]
        }
      ],
      "source": [
        "forecaster.fit(y=train[target_column_name], X=train.drop(columns=[target_column_name]), fh=fh)\n",
        "forecaster.predict(fh, test.drop(target_column_name))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Forecast<a id=\"forecast\"></a>\n",
        "\n",
        "Now that we have retrieved the best pipeline/model, it can be used to make predictions on test data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrieving forecasts from the model\n",
        "\n",
        "To produce predictions on the test set, we need to know the feature values at all dates in the test set. This requirement is somewhat reasonable for the OJ sales data since the features mainly consist of price, which is usually set in advance, and customer demographics which are approximately constant for each store over the 20 week forecast horizon in the testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Try downloading the model and running forecasts locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1671040294463
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "best_run.download_file('outputs/model.pkl')\n",
        "model = joblib.load('model.pkl')\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1671040765169
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# forecast returns the predictions for all specified quantiles\n",
        "# use 0.025 and 0.975 for the 95% confidence interval\n",
        "model.quantiles = [0.025, 0.5, 0.975]\n",
        "y_pred_quantiles = model.forecast_quantiles(test.loc[:, test.columns != target_column_name])\n",
        "y_pred_quantiles.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Responsible AI Dashboard<a id=\"analyze\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from raiwidgets import ResponsibleAIDashboard\n",
        "from responsibleai import RAIForecastingInsights, FeatureMetadata\n",
        "\n",
        "feature_metadata = FeatureMetadata(\n",
        "    time_series_id_column_names=time_series_id_column_names, \n",
        "    categorical_features=time_series_id_column_names,\n",
        "    time_column_name=time_column_name)\n",
        "insights = RAIForecastingInsights(\n",
        "    model=model,\n",
        "    train=train,\n",
        "    test=test,\n",
        "    target_column=target_column_name,\n",
        "    feature_metadata=feature_metadata)\n",
        "\n",
        "ResponsibleAIDashboard(insights)"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "jialiu"
      }
    ],
    "categories": [
      "SDK v1",
      "how-to-use-azureml",
      "automated-machine-learning"
    ],
    "category": "tutorial",
    "celltoolbar": "Raw Cell Format",
    "compute": [
      "Remote"
    ],
    "datasets": [
      "Orange Juice Sales"
    ],
    "deployment": [
      "Azure Container Instance"
    ],
    "exclude_from_index": false,
    "framework": [
      "Azure ML AutoML"
    ],
    "friendly_name": "Forecasting orange juice sales with deployment",
    "index_order": 1,
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "sktime",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "tags": [
      "None"
    ],
    "task": "Forecasting",
    "vscode": {
      "interpreter": {
        "hash": "6424d405450b15a93ca3015242fc1e51ac658b1b4015ae2fef5559269d9e1e0c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
