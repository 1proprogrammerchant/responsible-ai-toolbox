{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Responsible AI dashboard for Time Series Forecasting\n",
        "_**Orange Juice Sales Forecasting**_\n",
        "\n",
        "## Contents\n",
        "1. [Introduction](#introduction)\n",
        "1. [Data](#data)\n",
        "1. [Train](#train)\n",
        "1. [Forecast](#forecast)\n",
        "1. [Responsible AI Dashboard](#analyze)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction<a id=\"introduction\"></a>\n",
        "In this example, we use sktime to train, select, and operationalize a time-series forecasting model for multiple time-series.\n",
        "\n",
        "The examples in the follow code samples use the University of Chicago's Dominick's Finer Foods dataset to forecast orange juice sales. Dominick's was a grocery chain in the Chicago metropolitan area."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1670990788014
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sktime.forecasting.arima import AutoARIMA\n",
        "from sktime.forecasting.base import ForecastingHorizon\n",
        "from sktime.forecasting.model_selection import temporal_train_test_split\n",
        "from sktime.utils.plotting import plot_series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data<a id=\"data\"></a>\n",
        "You are now ready to load the historical orange juice sales data. We will load the CSV file into a plain pandas DataFrame; the time column in the CSV is called _WeekStarting_, so it will be specially parsed into the datetime type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1670990899201
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                               WeekStarting  Store feature Brand feature  \\\n",
            "Store Brand       WeekStarting                                             \n",
            "2     dominicks   1990-06-14     1990-06-14              2     dominicks   \n",
            "      minute.maid 1990-06-14     1990-06-14              2   minute.maid   \n",
            "      tropicana   1990-06-14     1990-06-14              2     tropicana   \n",
            "5     dominicks   1990-06-14     1990-06-14              5     dominicks   \n",
            "      minute.maid 1990-06-14     1990-06-14              5   minute.maid   \n",
            "\n",
            "                                Quantity  Advert  Price     Age60   COLLEGE  \\\n",
            "Store Brand       WeekStarting                                                \n",
            "2     dominicks   1990-06-14       10560       1   1.59  0.232865  0.248935   \n",
            "      minute.maid 1990-06-14        4480       0   3.17  0.232865  0.248935   \n",
            "      tropicana   1990-06-14        8256       0   3.87  0.232865  0.248935   \n",
            "5     dominicks   1990-06-14        1792       1   1.59  0.117368  0.321226   \n",
            "      minute.maid 1990-06-14        4224       0   2.99  0.117368  0.321226   \n",
            "\n",
            "                                   INCOME  Hincome150  Large HH  Minorities  \\\n",
            "Store Brand       WeekStarting                                                \n",
            "2     dominicks   1990-06-14    10.553205    0.463887  0.103953    0.114280   \n",
            "      minute.maid 1990-06-14    10.553205    0.463887  0.103953    0.114280   \n",
            "      tropicana   1990-06-14    10.553205    0.463887  0.103953    0.114280   \n",
            "5     dominicks   1990-06-14    10.922371    0.535883  0.103092    0.053875   \n",
            "      minute.maid 1990-06-14    10.922371    0.535883  0.103092    0.053875   \n",
            "\n",
            "                                WorkingWoman  SSTRDIST   SSTRVOL   CPDIST5  \\\n",
            "Store Brand       WeekStarting                                               \n",
            "2     dominicks   1990-06-14        0.303585  2.110122  1.142857  1.927280   \n",
            "      minute.maid 1990-06-14        0.303585  2.110122  1.142857  1.927280   \n",
            "      tropicana   1990-06-14        0.303585  2.110122  1.142857  1.927280   \n",
            "5     dominicks   1990-06-14        0.410568  3.801998  0.681818  1.600573   \n",
            "      minute.maid 1990-06-14        0.410568  3.801998  0.681818  1.600573   \n",
            "\n",
            "                                 CPWVOL5  \n",
            "Store Brand       WeekStarting            \n",
            "2     dominicks   1990-06-14    0.376927  \n",
            "      minute.maid 1990-06-14    0.376927  \n",
            "      tropicana   1990-06-14    0.376927  \n",
            "5     dominicks   1990-06-14    0.736307  \n",
            "      minute.maid 1990-06-14    0.736307  \n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Invalid fill method. Expecting pad (ffill) or backfill (bfill). Got linear",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m data\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mStore\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mStore feature\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBrand\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mBrand feature\u001b[39m\u001b[39m\"\u001b[39m}, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39mhead())\n\u001b[1;32m---> 12\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mgroupby(time_series_id_column_names)\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m group: group\u001b[39m.\u001b[39;49mloc[group\u001b[39m.\u001b[39;49mname]\u001b[39m.\u001b[39;49masfreq(\u001b[39m\"\u001b[39;49m\u001b[39mW-THU\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49minterpolate())\n\u001b[0;32m     13\u001b[0m data\u001b[39m.\u001b[39msort_index(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, ascending\u001b[39m=\u001b[39m[\u001b[39mTrue\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mTrue\u001b[39;00m])\n\u001b[0;32m     15\u001b[0m data\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1567\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1559\u001b[0m     new_msg \u001b[39m=\u001b[39m (\n\u001b[0;32m   1560\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe operation \u001b[39m\u001b[39m{\u001b[39;00morig_func\u001b[39m}\u001b[39;00m\u001b[39m failed on a column. If any error is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1561\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mraised, this will raise an exception in a future version \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1562\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof pandas. Drop these columns to avoid this warning.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1563\u001b[0m     )\n\u001b[0;32m   1564\u001b[0m     \u001b[39mwith\u001b[39;00m rewrite_warning(\n\u001b[0;32m   1565\u001b[0m         old_msg, \u001b[39mFutureWarning\u001b[39;00m, new_msg\n\u001b[0;32m   1566\u001b[0m     ) \u001b[39mif\u001b[39;00m is_np_func \u001b[39melse\u001b[39;00m nullcontext():\n\u001b[1;32m-> 1567\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_apply_general(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selected_obj)\n\u001b[0;32m   1568\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   1569\u001b[0m     \u001b[39m# gh-20949\u001b[39;00m\n\u001b[0;32m   1570\u001b[0m     \u001b[39m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1574\u001b[0m     \u001b[39m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m     \u001b[39m# on a string grouper column\u001b[39;00m\n\u001b[0;32m   1577\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_group_selection_context():\n\u001b[0;32m   1578\u001b[0m         \u001b[39m# GH#50538\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1629\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   1593\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_python_apply_general\u001b[39m(\n\u001b[0;32m   1594\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1599\u001b[0m     is_agg: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1600\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[0;32m   1601\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1602\u001b[0m \u001b[39m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1603\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1627\u001b[0m \u001b[39m        data after applying f\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1629\u001b[0m     values, mutated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49mapply(f, data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxis)\n\u001b[0;32m   1630\u001b[0m     \u001b[39mif\u001b[39;00m not_indexed_same \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1631\u001b[0m         not_indexed_same \u001b[39m=\u001b[39m mutated \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmutated\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:839\u001b[0m, in \u001b[0;36mBaseGrouper.apply\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39m# group might be modified\u001b[39;00m\n\u001b[0;32m    838\u001b[0m group_axes \u001b[39m=\u001b[39m group\u001b[39m.\u001b[39maxes\n\u001b[1;32m--> 839\u001b[0m res \u001b[39m=\u001b[39m f(group)\n\u001b[0;32m    840\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mutated \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    841\u001b[0m     mutated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[6], line 12\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(group)\u001b[0m\n\u001b[0;32m     10\u001b[0m data\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mStore\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mStore feature\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBrand\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mBrand feature\u001b[39m\u001b[39m\"\u001b[39m}, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39mhead())\n\u001b[1;32m---> 12\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mgroupby(time_series_id_column_names)\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m group: group\u001b[39m.\u001b[39;49mloc[group\u001b[39m.\u001b[39;49mname]\u001b[39m.\u001b[39;49masfreq(\u001b[39m\"\u001b[39;49m\u001b[39mW-THU\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49minterpolate())\n\u001b[0;32m     13\u001b[0m data\u001b[39m.\u001b[39msort_index(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, ascending\u001b[39m=\u001b[39m[\u001b[39mTrue\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mTrue\u001b[39;00m])\n\u001b[0;32m     15\u001b[0m data\u001b[39m.\u001b[39mhead(\u001b[39m10\u001b[39m)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\frame.py:11855\u001b[0m, in \u001b[0;36mDataFrame.interpolate\u001b[1;34m(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)\u001b[0m\n\u001b[0;32m  11843\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m  11844\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minterpolate\u001b[39m(\n\u001b[0;32m  11845\u001b[0m     \u001b[39mself\u001b[39m: DataFrame,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11853\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m  11854\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m> 11855\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49minterpolate(\n\u001b[0;32m  11856\u001b[0m         method,\n\u001b[0;32m  11857\u001b[0m         axis,\n\u001b[0;32m  11858\u001b[0m         limit,\n\u001b[0;32m  11859\u001b[0m         inplace,\n\u001b[0;32m  11860\u001b[0m         limit_direction,\n\u001b[0;32m  11861\u001b[0m         limit_area,\n\u001b[0;32m  11862\u001b[0m         downcast,\n\u001b[0;32m  11863\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m  11864\u001b[0m     )\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\generic.py:7568\u001b[0m, in \u001b[0;36mNDFrame.interpolate\u001b[1;34m(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)\u001b[0m\n\u001b[0;32m   7562\u001b[0m \u001b[39mif\u001b[39;00m isna(index)\u001b[39m.\u001b[39many():\n\u001b[0;32m   7563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   7564\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInterpolation with NaNs in the index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   7565\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhas not been implemented. Try filling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   7566\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthose NaNs before interpolating.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   7567\u001b[0m     )\n\u001b[1;32m-> 7568\u001b[0m new_data \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49minterpolate(\n\u001b[0;32m   7569\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m   7570\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   7571\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   7572\u001b[0m     limit\u001b[39m=\u001b[39;49mlimit,\n\u001b[0;32m   7573\u001b[0m     limit_direction\u001b[39m=\u001b[39;49mlimit_direction,\n\u001b[0;32m   7574\u001b[0m     limit_area\u001b[39m=\u001b[39;49mlimit_area,\n\u001b[0;32m   7575\u001b[0m     inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   7576\u001b[0m     downcast\u001b[39m=\u001b[39;49mdowncast,\n\u001b[0;32m   7577\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   7578\u001b[0m )\n\u001b[0;32m   7580\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\n\u001b[0;32m   7581\u001b[0m \u001b[39mif\u001b[39;00m should_transpose:\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\internals\\managers.py:422\u001b[0m, in \u001b[0;36mBaseBlockManager.interpolate\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minterpolate\u001b[39m(\u001b[39mself\u001b[39m: T, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m--> 422\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\u001b[39m\"\u001b[39;49m\u001b[39minterpolate\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(b, f)(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    353\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    354\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_failures:\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1619\u001b[0m, in \u001b[0;36mEABackedBlock.interpolate\u001b[1;34m(self, method, axis, inplace, limit, fill_value, **kwargs)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     new_values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mfillna(value\u001b[39m=\u001b[39mfill_value, method\u001b[39m=\u001b[39mmethod, limit\u001b[39m=\u001b[39mlimit)\u001b[39m.\u001b[39mT\n\u001b[0;32m   1618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1619\u001b[0m     new_values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39;49mfillna(value\u001b[39m=\u001b[39;49mfill_value, method\u001b[39m=\u001b[39;49mmethod, limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[0;32m   1620\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_block_same_class(new_values)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:317\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.fillna\u001b[1;34m(self, value, method, limit)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39m@doc\u001b[39m(ExtensionArray\u001b[39m.\u001b[39mfillna)\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfillna\u001b[39m(\n\u001b[0;32m    315\u001b[0m     \u001b[39mself\u001b[39m: NDArrayBackedExtensionArrayT, value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, method\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    316\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDArrayBackedExtensionArrayT:\n\u001b[1;32m--> 317\u001b[0m     value, method \u001b[39m=\u001b[39m validate_fillna_kwargs(\n\u001b[0;32m    318\u001b[0m         value, method, validate_scalar_dict_value\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    319\u001b[0m     )\n\u001b[0;32m    321\u001b[0m     mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39misna()\n\u001b[0;32m    322\u001b[0m     \u001b[39m# error: Argument 2 to \"check_value_size\" has incompatible type\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[39m# \"ExtensionArray\"; expected \"ndarray\"\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\util\\_validators.py:390\u001b[0m, in \u001b[0;36mvalidate_fillna_kwargs\u001b[1;34m(value, method, validate_scalar_dict_value)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMust specify a fill \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    389\u001b[0m \u001b[39melif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m     method \u001b[39m=\u001b[39m clean_fill_method(method)\n\u001b[0;32m    392\u001b[0m \u001b[39melif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m     \u001b[39mif\u001b[39;00m validate_scalar_dict_value \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(value, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\sktime\\lib\\site-packages\\pandas\\core\\missing.py:125\u001b[0m, in \u001b[0;36mclean_fill_method\u001b[1;34m(method, allow_nearest)\u001b[0m\n\u001b[0;32m    123\u001b[0m     expecting \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpad (ffill), backfill (bfill) or nearest\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m valid_methods:\n\u001b[1;32m--> 125\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid fill method. Expecting \u001b[39m\u001b[39m{\u001b[39;00mexpecting\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mmethod\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[39mreturn\u001b[39;00m method\n",
            "\u001b[1;31mValueError\u001b[0m: Invalid fill method. Expecting pad (ffill) or backfill (bfill). Got linear"
          ]
        }
      ],
      "source": [
        "time_column_name = \"WeekStarting\"\n",
        "time_series_id_column_names = [\"Store\", \"Brand\"]\n",
        "dataset_location = \"https://raw.githubusercontent.com/Azure/azureml-examples/2fe81643865e1f4591e7734bd1a729093cafb826/v1/python-sdk/tutorials/automl-with-azureml/forecasting-orange-juice-sales/dominicks_OJ.csv\"\n",
        "data = pd.read_csv(dataset_location, parse_dates=[time_column_name])\n",
        "\n",
        "# Drop the columns 'logQuantity' as it is a leaky feature.\n",
        "data.drop(\"logQuantity\", axis=1, inplace=True)\n",
        "\n",
        "data.set_index(time_series_id_column_names + [time_column_name], inplace=True, drop=False)\n",
        "data.rename(columns={\"Store\": \"Store feature\", \"Brand\": \"Brand feature\"}, inplace=True)\n",
        "data = data.groupby(time_series_id_column_names).apply(lambda group: group.loc[group.name].asfreq(\"W-THU\").interpolate())\n",
        "data.sort_index(inplace=True, ascending=[True, True, True])\n",
        "\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each row in the DataFrame holds a quantity of weekly sales for an OJ brand at a single store. The data also includes the sales price, a flag indicating if the OJ brand was advertised in the store that week, and some customer demographic information based on the store location. For historical reasons, the data also include the logarithm of the sales quantity. The Dominick's grocery data is commonly used to illustrate econometric modeling techniques where logarithms of quantities are generally preferred.    \n",
        "\n",
        "The task is now to build a time-series model for the _Quantity_ column. It is important to note that this dataset is comprised of many individual time-series - one for each unique combination of _Store_ and _Brand_. To distinguish the individual time-series, we define the **time_series_id_column_names** - the columns whose values determine the boundaries between time-series: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670990902872
        }
      },
      "outputs": [],
      "source": [
        "nseries = data.groupby(time_series_id_column_names).ngroups\n",
        "print(\"Data contains {0} individual time-series.\".format(nseries))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For demonstration purposes, we extract sales time-series for just a few of the stores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670990905562
        }
      },
      "outputs": [],
      "source": [
        "use_stores = [2, 5, 8]\n",
        "data_subset = data.loc[([2,5,8], slice(None), slice(None)), :]\n",
        "nseries = data_subset.groupby(time_series_id_column_names).ngroups\n",
        "print(\"Data subset contains {0} individual time-series.\".format(nseries))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Splitting\n",
        "We now split the data into a training and a testing set for later forecast evaluation. The test set will contain the final 20 weeks of observed sales for each time-series. The splits should be stratified by series, so we use a group-by statement on the time series identifier columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670990907583
        }
      },
      "outputs": [],
      "source": [
        "target_column_name = \"Quantity\"\n",
        "\n",
        "y = pd.DataFrame(data_subset[target_column_name])\n",
        "X = data_subset.drop(columns=[target_column_name])\n",
        "fh_dates = pd.DatetimeIndex(y.index.get_level_values(2).unique().sort_values().to_list()[-20:], freq='W-THU')\n",
        "fh = ForecastingHorizon(fh_dates, is_relative=False)\n",
        "y_train, y_test, X_train, X_test = \\\n",
        "    temporal_train_test_split(\n",
        "        y=y,\n",
        "        X=X,\n",
        "        test_size=20)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modeling\n",
        "\n",
        "For forecasting tasks, we need to do several pre-processing and estimation steps that are specific to time-series including:\n",
        "* Detect time-series sample frequency (e.g. hourly, daily, weekly) and create new records for absent time points to make the series regular. A regular time series has a well-defined frequency and has a value at every sample point in a contiguous time span \n",
        "* Impute missing values in the target (via forward-fill) and feature columns (using median column values) \n",
        "* Create features based on time series identifiers to enable fixed effects across different series\n",
        "* Create time-based features to assist in learning seasonal patterns\n",
        "* Encode categorical variables to numeric quantities\n",
        "\n",
        "In this notebook, we will train a single, regression-type model across **all** time-series in a given training set. This allows the model to generalize across related series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Customization\n",
        "\n",
        "The featurization customization in forecasting is an advanced feature in AutoML which allows our customers to change the default forecasting featurization behaviors and column types through `FeaturizationConfig`. The supported scenarios include:\n",
        "\n",
        "1. Column purposes update: Override feature type for the specified column. Currently supports DateTime, Categorical and Numeric. This customization can be used in the scenario that the type of the column cannot correctly reflect its purpose. Some numerical columns, for instance, can be treated as Categorical columns which need to be converted to categorical while some can be treated as epoch timestamp which need to be converted to datetime. To tell our SDK to correctly preprocess these columns, a configuration need to be add with the columns and their desired types.\n",
        "2. Transformer parameters update: Currently supports parameter change for Imputer only. User can customize imputation methods. The supported imputing methods for target column are constant and ffill (forward fill). The supported imputing methods for feature columns are mean, median, most frequent, constant and ffill (forward fill). This customization can be used for the scenario that our customers know which imputation methods fit best to the input data. For instance, some datasets use NaN to represent 0 which the correct behavior should impute all the missing value with 0. To achieve this behavior, these columns need to be configured as constant imputation with `fill_value` 0.\n",
        "3. Drop columns: Columns to drop from being featurized. These usually are the columns which are leaky or the columns contain no useful data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670990927322
        },
        "tags": [
          "sample-featurizationconfig-remarks"
        ]
      },
      "outputs": [],
      "source": [
        "#forecaster = TransformedTargetForecaster(steps=[\n",
        "#    # (\"detrend\", Detrender()),\n",
        "#    # (\"deseasonalize\", Deseasonalizer()),\n",
        "#    # (\"minmax\", TabularToSeriesAdaptor(MinMaxScaler((1, 10)))),\n",
        "#    # (\"power\", TabularToSeriesAdaptor(PowerTransformer())),\n",
        "#    # (\"scaler\", TabularToSeriesAdaptor(RobustScaler())),\n",
        "#    (\"forecaster\", DirectTabularRegressionForecaster(KNeighborsTimeSeriesRegressor())),\n",
        "#])\n",
        "\n",
        "\"\"\"\n",
        "featurization_config = FeaturizationConfig()\n",
        "# Force the CPWVOL5 feature to be numeric type.\n",
        "featurization_config.add_column_purpose(\"CPWVOL5\", \"Numeric\")\n",
        "# Fill missing values in the target column, Quantity, with zeros.\n",
        "featurization_config.add_transformer_params(\n",
        "    \"Imputer\", [\"Quantity\"], {\"strategy\": \"constant\", \"fill_value\": 0}\n",
        ")\n",
        "# Fill missing values in the INCOME column with median value.\n",
        "featurization_config.add_transformer_params(\n",
        "    \"Imputer\", [\"INCOME\"], {\"strategy\": \"median\"}\n",
        ")\n",
        "# Fill missing values in the Price column with forward fill (last value carried forward).\n",
        "featurization_config.add_transformer_params(\"Imputer\", [\"Price\"], {\"strategy\": \"ffill\"})\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forecasting Parameters\n",
        "To define forecasting parameters for your experiment training, you can leverage the ForecastingParameters class. The table below details the forecasting parameter we will be passing into our experiment.\n",
        "\n",
        "\n",
        "|Property|Description|\n",
        "|-|-|\n",
        "|**time_column_name**|The name of your time column.|\n",
        "|**forecast_horizon**|The forecast horizon is how many periods forward you would like to forecast. This integer horizon is in units of the timeseries frequency (e.g. daily, weekly).|\n",
        "|**time_series_id_column_names**|The column names used to uniquely identify the time series in data that has multiple rows with the same timestamp. If the time series identifiers are not defined, the data set is assumed to be one time series.|\n",
        "|**freq**|Forecast frequency. This optional parameter represents the period with which the forecast is desired, for example, daily, weekly, yearly, etc. Use this parameter for the correction of time series containing irregular data points or for padding of short time series. The frequency needs to be a pandas offset alias. Please refer to [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects) for more information.\n",
        "|**cv_step_size**|Number of periods between two consecutive cross-validation folds. The default value is \"auto\", in which case AutoMl determines the cross-validation step size automatically, if a validation set is not provided. Or users could specify an integer value."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train<a id=\"train\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now submit a new training run. Depending on the data and number of iterations this operation may take several minutes.\n",
        "Information from each iteration will be printed to the console.  Validation errors and current status will be shown when setting `show_output=True` and the execution will be synchronous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "forecaster = AutoARIMA(suppress_warnings=True, error_action=\"ignore\")\n",
        "forecaster.fit(y=y_train, X=X_train, fh=fh)\n",
        "forecaster.predict(fh=fh, X=X_test).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "forecaster.predict_quantiles(fh=fh, X=X_test, alpha=[0.025, 0.975]).head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Forecast<a id=\"forecast\"></a>\n",
        "\n",
        "Now that we have retrieved the best pipeline/model, it can be used to make predictions on test data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrieving forecasts from the model\n",
        "\n",
        "To produce predictions on the test set, we need to know the feature values at all dates in the test set. This requirement is somewhat reasonable for the OJ sales data since the features mainly consist of price, which is usually set in advance, and customer demographics which are approximately constant for each store over the 20 week forecast horizon in the testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Try downloading the model and running forecasts locally."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Responsible AI Dashboard<a id=\"analyze\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from raiwidgets import ResponsibleAIDashboard\n",
        "from responsibleai import RAIInsights, FeatureMetadata\n",
        "\n",
        "# merge X and y into a single DataFrame\n",
        "train = X_train.join(y_train)\n",
        "test = X_test.join(y_test)\n",
        "\n",
        "feature_metadata = FeatureMetadata(\n",
        "    time_series_id_features=time_series_id_column_names, \n",
        "    categorical_features=time_series_id_column_names,\n",
        "    datetime_features=[time_column_name])\n",
        "insights = RAIInsights(\n",
        "    model=forecaster,\n",
        "    train=train,\n",
        "    test=test,\n",
        "    task_type=\"forecasting\",\n",
        "    target_column=target_column_name,\n",
        "    feature_metadata=feature_metadata)\n",
        "\n",
        "ResponsibleAIDashboard(insights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "jialiu"
      }
    ],
    "categories": [
      "SDK v1",
      "how-to-use-azureml",
      "automated-machine-learning"
    ],
    "category": "tutorial",
    "celltoolbar": "Raw Cell Format",
    "compute": [
      "Remote"
    ],
    "datasets": [
      "Orange Juice Sales"
    ],
    "deployment": [
      "Azure Container Instance"
    ],
    "exclude_from_index": false,
    "framework": [
      "Azure ML AutoML"
    ],
    "friendly_name": "Forecasting orange juice sales with deployment",
    "index_order": 1,
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "sktime",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "tags": [
      "None"
    ],
    "task": "Forecasting",
    "vscode": {
      "interpreter": {
        "hash": "6424d405450b15a93ca3015242fc1e51ac658b1b4015ae2fef5559269d9e1e0c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
